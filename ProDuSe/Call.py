#! /usr/bin/env python

import argparse
import os
import pysam
import sys
import sortedcontainers
from pyfaidx import Fasta
import time
from fisher import pvalue
from configobj import ConfigObj
import bisect

class Position:
    """
    Stores the allele counts and associated characteristics at this position
    """

    def __init__(self, refBase):
        self.ref = refBase
        self.alleles = []
        self.qualities = []
        self.famSizes = []
        self.posParent = []
        self.mapStrand = []
        self.distToEnd = []
        self.mismatchNum = []
        self.mappingQual = []
        self.alt = False
        self.altAlleles = set()
        self.readNum = []

        self.supportsDel = False
        self.delPassedFilters = False
        self.delFailReason = None

        self.consensusIns = []
        self.insPassedFilters = False
        self.insFailReason = "PASS"

    def add(self, base, qual, size, posParent, mapStrand, dist, mapQual, readNum):

        self.alleles.append(base)
        self.qualities.append(qual)
        self.famSizes.append(size)
        self.posParent.append(posParent)
        self.mapStrand.append(mapStrand)
        self.distToEnd.append(dist)
        self.mappingQual.append(mapQual)
        self.readNum.append(readNum)

        if self.ref != "N" and self.ref != base:
            self.alt = True
            if base not in self.altAlleles:
                self.altAlleles.add(base)
            return True
        else:
            return False

    def addMismatchNum(self, misMatchNum):
        self.mismatchNum.append(misMatchNum)

    def processVariant(self, refWindow, nearbyVariants, nWindow):
        self.refWindow = refWindow
        self.nearbyVar = nearbyVariants
        self.nWindow = nWindow  # How many positions were examined to find nearby variants?

    def summarizeVariant(self, minDepth=0, minAltDepth=0, strandBiasThreshold=0, strongMoleculeThreshold=3):
        """
        Summarize the statistics of this position
        :return:
        """

        self.baseCounts = { "DPN":  [0, 0, 0, 0, 0],
                            "DpN":  [0, 0, 0, 0, 0],
                            "DPn":  [0, 0, 0, 0, 0],
                            "Dpn":  [0, 0, 0, 0, 0],
                            "SN" :  [0, 0, 0, 0, 0],
                            "SP" :  [0, 0, 0, 0, 0],
                            "Sp" :  [0, 0, 0, 0, 0],
                            "Sn" :  [0, 0, 0, 0, 0]
                            }
        self.strandCounts = [0.0, 0.0, 0.0, 0.0, 0.0]
        self.posMolec = [0, 0, 0, 0, 0]
        self.negMolec = [0, 0, 0, 0, 0]

        baseToIndex = {"A": 0, "C": 1, "G": 2, "T": 3, "-": 4}
        refIndex = baseToIndex[self.ref]
        pMapStrand = [0, 0, 0, 0, 0]
        nMapStrand = [0, 0, 0, 0, 0]

        depth = 0.0
        # What is the maximum quality of the alternate allele?
        self.maxQual = [0, 0, 0, 0, 0]

        # We need to identify and flag reads that are in duplex
        # Count the number of times each read number "A counter that is unique to each read, generated by collapse" occurs
        # If it occurs once, the read is a singleton. If it occurs twice, it is a duplex
        # That said, we also need to handle the case whereby clipoverlap was NOT run on the input BAM file. A read pair
        # will have the same number
        # To deal with this, create a dictionary which will store both the read, the parental strand, and index
        readOrigin = {}

        # Since the characteristics of each read is in order (i.e. index 0 corresponds to the first read that overlaps
        # this position for every list, cycle through all stored attributes
        for base, qual, fSize, posParent, map, distToEnd, mismatchNum, mapQual, readID \
                in zip(self.alleles, self.qualities, self.famSizes, self.posParent, self.mapStrand, self.distToEnd, self.mismatchNum, self.mappingQual, self.readNum):

            try:
                baseIndex = baseToIndex[base]
            except KeyError:

                # Handle insertions
                if len(base) > 1:
                    baseIndex = 4
                    baseToIndex[base] = 4

                else:
                    # i.e. There is an odd base here. Probably an "N". In this case, ignore it
                    continue

            # Handle insertions
            if len(base) > 1:
                qual = qual[0]
                map = map[0]

                # Process the bases of this insertion
                i = 0
                for b in base:
                    if i == len(self.consensusIns):
                        self.consensusIns.append({b: 1})
                    elif base not in self.consensusIns[i]:
                        self.consensusIns[i][b] = 1
                    else:
                        self.consensusIns[i][b] += 1
                    i += 1

            if qual > self.maxQual[baseIndex]:
                self.maxQual[baseIndex] = qual
            # What type of read is this, in terms of parental strand
            if posParent:
                if fSize >= strongMoleculeThreshold:
                    pType = "P"
                else:
                    pType = "p"
                self.posMolec[baseIndex] += 1
            else:
                if fSize >= strongMoleculeThreshold:
                    pType = "N"
                else:
                    pType = "n"
                self.negMolec[baseIndex] += 1

            # Calculate Strand bias info
            if map == "S":  # This base is a consensus from both the strand mapped to the (+) and (-) strand
                pMapStrand[baseIndex] += 1
                nMapStrand[baseIndex] += 1
            elif map== "F":
                pMapStrand[baseIndex] += 1
            else:
                nMapStrand[baseIndex] += 1

            # Check to see if there is another read that is the duplex mate of this read
            key = readID + ":" + str(baseIndex)
            if key in readOrigin:
                readOrigin[key] = readOrigin[key] + pType
            else:
                readOrigin[key] = pType
                self.strandCounts[baseIndex] += 1
                # Total molecule count (for VAF)
                depth += 1

        # Generate a count of each parental molecule at this position
        for name, pStrand in readOrigin.items():
            baseIndex = int(name.split(":")[1])
            if pStrand == "N":
                self.baseCounts["SN"][baseIndex] += 1
            elif pStrand == "P":
                self.baseCounts["SP"][baseIndex] += 1
            elif pStrand == "n":
                self.baseCounts["Sn"][baseIndex] += 1
            elif pStrand == "p":
                self.baseCounts["Sp"][baseIndex] += 1
            elif pStrand == "PN" or pStrand == "NP":
                self.baseCounts["DPN"][baseIndex] += 1
            elif pStrand == "Pn" or pStrand == "nP":
                self.baseCounts["DPn"][baseIndex] += 1
            elif pStrand == "pN" or pStrand == "Np":
                self.baseCounts["DpN"][baseIndex] += 1
            elif pStrand == "pn" or pStrand == "np":
                self.baseCounts["Dpn"][baseIndex] += 1
            else:
                raise TypeError("It seems the developer messed up. You should send him an angry e-mail! (Mention this message please)")

        # Calculate maximum alt quality
        self.maxAltQual = max(self.maxQual[baseToIndex[x]] for x in self.altAlleles)
        self.maxPassedAltQual = None

        # Calculate strand bias
        self.strandBias = []
        for index in range(0, 5):
            if pMapStrand[index] + nMapStrand[index] == 0:
                self.strandBias.append(1.0)
            else:
                self.strandBias.append(pvalue(pMapStrand[index], nMapStrand[index], pMapStrand[refIndex], nMapStrand[refIndex]).two_tail)

        self.vafs = []
        for index in range(0,5):
            # baseCount = 0.0
            # for molecType in self.baseCounts.values():
            #     baseCount += molecType[index]
            self.vafs.append(self.strandCounts[index]/depth)

        # Does this variant fail basic filters?
        failedFilters = []
        self.passedAltAlleles = []
        if depth < minDepth:
            failedFilters.append("LOW_DEPTH")
        else:
            for allele in self.altAlleles:

                if len(allele) > 1:
                    isIns = True
                else:
                    isIns = False
                # If this allele is an insertion, we will handle this seperately
                index = baseToIndex[allele]
                # Does this alternate allele fail depth filters
                if self.strandCounts[index] < minAltDepth:
                    if allele == "-":
                        self.delFailReason = "LOW_ALT_SUPPORT"
                    elif isIns:
                        self.insFailReason = "LOW_ALT_SUPPORT"
                    else:
                        failedFilters.append("LOW_ALT_SUPPORT")
                    continue

                # Does this alt allele fail strand bias filters?
                if self.strandBias[index] < strandBiasThreshold:
                    if allele == "-":
                        self.delFailReason = "STRAND_BIAS"
                    elif isIns:
                        self.insFailReason = "STRAND_BIAS"
                    else:
                        failedFilters.append("STRAND_BIAS")
                    continue

                # If the alternate allele which passed filters is a deletion, flag it here
                if allele == "-":
                    self.delPassedFilters = True
                elif isIns:
                    self.insPassedFilters = True
                else:
                    self.passedAltAlleles.append(allele)

            # Does this position support a deletion? If so, flag it as such
            if "-" in self.altAlleles:
                self.supportsDel = True
                self.altAlleles.remove("-")

            # There are still alleles left after filtering. The variant has passed filters
            if len(self.passedAltAlleles) != 0:
                self.maxPassedAltQual = max(self.maxQual[baseToIndex[x]] for x in self.passedAltAlleles)
                failedFilters = []

        # If there is a deletion, generate a consensus from it now
        if len(self.consensusIns) != 0:
            consensus = []
            insWeight = sum(x for x in self.consensusIns[0].values())  # TODO: Handle this in a less stupid manner
            for position in self.consensusIns:
                maxWeight = 0
                maxBase = None
                posWeight = 0
                for base, weight in position.items():
                    if weight > maxWeight:  # TODO: Consider quality score
                        maxBase = base
                        maxWeight = weight
                    posWeight += weight
                if insWeight / 2 > posWeight:
                    break
                consensus.append(maxBase)

            self.consensusIns = consensus

        return failedFilters


class PileupEngine:
    """
    Generates a custom pileup using family characteristics
    """
    def __init__(self, inBAM, refGenome, targetRegions, homopolymerWindow=5, noiseWindow=150, pileupWindow = 1000, printPrefix="PRODUSE-CALL\t"):
        self._inFile=pysam.AlignmentFile(inBAM)
        self._refGenome = Fasta(refGenome, read_ahead=20000)
        self._captureSpace = self._loadCaptureSpace(targetRegions)
        self._candidateIndels = []
        self.pileup = {}
        self.candidateVar = {}
        self.filteredVar = {}
        self._homopolymerWindow = homopolymerWindow
        self._noiseWindow = noiseWindow
        # The number of positions to store before collapsing previous positions
        # Reduce this number to lower memory footprint, but be warned that, if this falls below the length of a read,
        # Some positions may not be tallied correctly
        self._pileupWindow = pileupWindow

        self._printPrefix = printPrefix

    def _loadCaptureSpace(self, file):
        """
        Parse genomic regions from a specified BED file, and load them into a dictionary

        :param file: A string containing a filepath to the BED file which specifies the capture space
        :return: A dictionary listing "chrom" : (start, end, start, end, start, end)
        """

        if file is None:
            return None

        # Does the BAM file use "chr"-prefixed contig names? If so, we should make sure the names of these regions are consistent
        isChrPrefixed = None
        try:
            # Check the first reference sequence listed in the BAM header. This isn't perfect, but it should handle most cases
            isChrPrefixed = self._inFile.header["SQ"][0]["SN"].startswith("chr")
        except AssertionError:
            pass

        try:
            targets = {}
            with open(file) as f:
                for line in f:
                    elements = line.split("\t")
                    chrom = elements[0]
                    # Make sure the "chr" prefix used here is consistent with the BAM file
                    if isChrPrefixed is not None:
                        if isChrPrefixed:
                            if not chrom.startswith("chr"):
                                chrom = "chr" + chrom
                        else:
                            if chrom.startswith("chr"):
                                chrom = chrom[3:]

                    # Since BED files use 1-based numbering, while pysam uses 0, include an offset
                    start = int(elements[1]) - 1
                    end = int(elements[2]) - 1
                    if chrom not in targets:
                        targets[chrom] = []
                    targets[chrom].extend([start, end])
                    # Ensure that the BED interval is actually valid
                    if start > end:
                        sys.stderr.write(
                            "ERROR: The start position \'%s\' is greater than the end position \'%s\'\n" % (
                            start, end))
                        exit(1)
            # Finally, to speed up access, convert each list into a tuple
            for chrom, locations in targets.items():
                targets[chrom] = tuple(locations)
            return targets
        except (IndexError, ValueError):
            sys.stderr.write("ERROR: Unable to parse BED interval \'%s\'\n" % (line.strip("\n").strip("\r")))
            sys.stderr.write("Ensure the file is a valid BED file, and try again\n")
            exit(1)

    def _insideCaptureSpace(self, read):  # TODO: Once INDELS are handled, reconsider this to include reads which may contain an INDEL at the capture space boundry
        """
        Does this read fall within our capture space? Note that this excludes soft-clipping
        :param read: A pysam.AlignedSegment() object, coresponding to the read of interest
        :return: A boolean indicating if this read overlaps the capture space
        """
        # If no capture space was specified, then considering this read "within" the capture space
        if self._captureSpace is None:
            return True
        # Check Chromosome
        if read.reference_name not in self._captureSpace:
            return False

        # Check read coordinates
        bisectPoint1 = bisect.bisect_left(self._captureSpace[read.reference_name], read.reference_start)
        bisectPoint2 = bisect.bisect_left(self._captureSpace[read.reference_name], read.reference_end)
        if bisectPoint1 == bisectPoint2 and bisectPoint1 % 2 != 1:
            return False
        return True

    def __writeVcfHeader(self, file):
        """
        Prints a VCF header to the specified file

        :param file: A file object, open to writing
        """

        header = ["##fileformat=VCFv4.3",
                  "##reference=" + self._refGenome.filename]

        # Add the contig information
        for contig, features in self._refGenome.faidx.index.items():
            line = "##contig=<ID=" + contig + ",length=" + str(len(features)) + ">"
            header.append(line)

        # Add the info field info
        header.append('##INFO=<ID=DPN,Number=R,Type=Integer,Description="Duplex Support with Strong Positive and Strong Negative Consensus">')
        header.append('##INFO=<ID=DPn,Number=R,Type=Integer,Description="Duplex Support with Strong Positive and Weak Negative Consensus">')
        header.append('##INFO=<ID=DpN,Number=R,Type=Integer,Description="Duplex Support with Weak Positive and Strong Negative Consensus">')
        header.append('##INFO=<ID=Dpn,Number=R,Type=Integer,Description="Duplex Support with Weak Positive and Weak Negative Consensus">')
        header.append('##INFO=<ID=SP,Number=R,Type=Integer,Description="Singleton Support with Strong Positive Consensus">')
        header.append('##INFO=<ID=Sp,Number=R,Type=Integer,Description="Singleton Support with Weak Positive Consensus">')
        header.append('##INFO=<ID=SN,Number=R,Type=Integer,Description="Singleton Support with Strong Negative Consensus">')
        header.append('##INFO=<ID=MC,Number=R,Type=Integer,Description="Total Molecule counts for Each Allele">')
        header.append('##INFO=<ID=STP,Number=R,Type=Float,Description="Probability of Strand Bias during sequencing for each Allele">')
        header.append('##INFO=<ID=PC,Number=R,Type=Integer,Description="Positive Strand Molecule Counts">')
        header.append('##INFO=<ID=NC,Number=R,Type=Integer,Description="Negative Strand Molecule Counts">')
        header.append('##INFO=<ID=VAF,Number=R,Type=Float,Description="Variant allele fraction of alternate allele(s) at this locus">')

        header.append("\t".join(["#CHROM", "POS", "ID", "REF", "ALT", "QUAL", "FILTER", "INFO\n"]))
        file.write(os.linesep.join(header))

    def _processDeletion(self, delPos, moleculeTypes, chrom, position):
        """
        Merge positions that support a deletion into a consensus
        :param delPos: An iterable containing one or more position objects which overlap the deletion
        :return: outLine: A string representing a VCF entry for this deletion
        """
        def _generateInfoCol(startPos, endPos):
            infoCol = []
            for molecule in moleculeTypes:
                x = molecule + "=" + ",".join(
                    str(int((x + y) / 2)) for x, y in zip(startPos.baseCounts[molecule], endPos.baseCounts[molecule]))
                infoCol.append(x)

            infoCol.append("MC=" + ",".join(str(int((x + y) / 2)) for x, y in
                                            zip(startPos.strandCounts, endPos.strandCounts)))  # Parental Strand counts
            infoCol.append("STP=" + ",".join(
                str((x + y) / 2) for x, y in zip(startPos.strandBias, endPos.strandBias)))  # Strand bias
            infoCol.append("PC=" + ",".join(str((x + y) / 2) for x, y in zip(startPos.posMolec, endPos.posMolec)))
            infoCol.append("NC=" + ",".join(str((x + y) / 2) for x, y in zip(startPos.negMolec, endPos.negMolec)))
            infoCol.append("VAF=" + ",".join(str((x + y) / 2) for x, y in zip(startPos.vafs, endPos.vafs)))  # VAF
            return infoCol


        # My general strategy here is to use an average of the boundries of the deletion to obtain molecule counts

        # First, generate the reference base
        refSeq = "".join(x.ref for x in delPos)


        # Next, average the molecule counts on either side of the deletion, and use those for the output
        # Generate the info column
        infoCol = _generateInfoCol(delPos[0], delPos[-1])

        # Next, process the portion of this deletion which passed filters
        startIndex = 0
        endIndex = len(delPos) - 1
        delLength = len(delPos)
        while not delPos[startIndex].delPassedFilters and startIndex < delLength - 1:
            startIndex += 1
        while not delPos[endIndex].delPassedFilters and endIndex >= 0:
            endIndex -= 1
        endIndex += 1

        # If this deletion failed filters, we don't need to do anything else except state the reason why it failed filters
        if startIndex >= endIndex:
            # Use the reason listed at the first position
            fail = delPos[0].delFailReason
        else:
            fail = "PASS"
        rawOutLine = "\t".join([chrom, str(position + 1), ".", refSeq, "-", ".", fail, ";".join(infoCol)])

        # If this variant passed filters, we need to generate a new record for the passed deletion if it is not the same
        # as the old one
        if fail != "PASS":
            passedOutLine = None
        elif startIndex == 0 and endIndex == len(delPos):
            passedOutLine = rawOutLine
        else:
            refSeq = "".join(x.ref for x in delPos[startIndex:endIndex])
            infoCol = _generateInfoCol(delPos[startIndex], delPos[endIndex - 1])
            passedOutLine = "\t".join([chrom, str(position + startIndex + 1), ".", refSeq, "-", ".", "PASS", ";".join(infoCol)])

        return rawOutLine, passedOutLine


    def filterAndWriteVariants(self, outfile, unfilteredOut=None, minMolecules=3, minAltMolecules=2, strandBiasThreshold=0.05):
        """

        Filters candidate variants based upon specified characteristics, and writes the output to the output file
        This is a placeholder for the real filter, which will be developed at a later time

        :param outfile: A string containing a filepath to the output VCF file
        :param unfilteredOut: A sting containing a filepath to an output VCF file which will contain ALL variants, even those that do not pass filters
        :param minMolecules: An integer indicating the minimum number of total molecules must cover a position to pass filters
        :param minAltMolecules: An integer indicating the minimum number of molecules must support a variant for it to pass filters
        :param strandBiasThreshold: A float representing the strand bias p-value threshold
        :return:
        """

        sys.stderr.write(
            "\t".join([self._printPrefix, time.strftime('%X'), "Filtering Candidate Variants\n"]))
        moleculeTypes = ("DPN", "DPn", "DpN", "Dpn", "SP", "Sp", "SN", "Sn")
        with open(outfile, "w") as o, open(unfilteredOut, "w") if unfilteredOut is not None else open(os.devnull, "w") as u:
            self.__writeVcfHeader(o)
            self.__writeVcfHeader(u)

            for chrom in self.candidateVar:
                # Merge adjacent positions which support a deletion with one another
                # TODO: Make this implemetation less hacky
                isCurrentDel = False
                currentDelLoc = None
                currentDelChrom = chrom
                currentDelPos = []
                for position, stats in self.candidateVar[chrom].items():

                    # try:
                    filtersFailed = stats.summarizeVariant(minMolecules, minAltMolecules, strandBiasThreshold)

                    # A zerodivisionerror will be thrown if the "depth" at this position is 0
                    # This will occur if all the bases which map to a position are "N"s
                    # A keyerror will occur if the Reference base is not an "A,C,G,T" (i.e. an "N" or "M")
                    # except (ZeroDivisionError, KeyError):
                    #     continue

                    # If the only variant at this position is a deletion, store it for later
                    if not isCurrentDel and stats.supportsDel:
                        isCurrentDel = True
                        currentDelPos.append(stats)
                        currentDelChrom = chrom
                        currentDelLoc = position
                    elif isCurrentDel:
                        if not stats.supportsDel:  # i.e. The previous positions supported a deletion, but this one does not. Process the deletion
                            rawOutLine, passedOutLine = self._processDeletion(currentDelPos, moleculeTypes, currentDelChrom, currentDelLoc)
                            u.write(rawOutLine + os.linesep)
                            if passedOutLine is not None:
                                o.write(passedOutLine + os.linesep)
                            currentDelPos = []
                            isCurrentDel = False
                        else:
                            currentDelPos.append(stats)

                    # If the only alt allele was a deletion, don't process this position yet
                    if len(stats.altAlleles) == 0:
                        continue

                    # Generate the info column
                    infoCol = []
                    for molecule in moleculeTypes:
                        x = molecule + "=" + ",".join(str(x) for x in stats.baseCounts[molecule])
                        infoCol.append(x)

                    infoCol.append("MC=" + ",".join(str(int(x)) for x in stats.strandCounts))  # Parental Strand counts
                    infoCol.append("STP=" + ",".join(str(x) for x in stats.strandBias))  # Strand bias
                    infoCol.append("PC=" + ",".join(str(x) for x in stats.posMolec))
                    infoCol.append("NC=" + ",".join(str(x) for x in stats.negMolec))
                    infoCol.append("VAF=" + ",".join(str(x) for x in stats.vafs))  # VAF

                    failedFilters = False
                    if len(filtersFailed) == 0:
                        filter = "PASS"
                    else:
                        filter = ",".join(filtersFailed)
                        failedFilters = True

                    # Add 1 to the position, to compensate for the fact that pysam uses 0-based indexing, while VCF files use 1-based
                    outLine = "\t".join([chrom, str(position + 1), ".", stats.ref, ",".join(stats.altAlleles), str(stats.maxAltQual), filter, ";".join(infoCol)])
                    u.write(outLine + os.linesep)
                    if not failedFilters:
                        outLine = "\t".join([chrom, str(position + 1), ".", stats.ref, ",".join(stats.passedAltAlleles),
                                             str(stats.maxPassedAltQual), filter, ";".join(infoCol)])
                        o.write(outLine + os.linesep)

                    # If there is an insertion at this position, we need to generate a record for that
                    if len(stats.consensusIns) > 0:
                        outLine = "\t".join([chrom, str(position + 1), ".", stats.ref, "".join(stats.consensusIns),
                                             ".", stats.insFailReason, ";".join(infoCol)])
                        u.write(outLine + os.linesep)
                        if stats.insFailReason == "PASS":
                            o.write(outLine)

        sys.stderr.write(
            "\t".join([self._printPrefix, time.strftime('%X'), "Variant Calling Complete\n"]))

    def cigarToTuple(self, cigarTuples):
        """
        Converts a pysam-style cigar tuples into a regular tuple, where 1 operator = 1 base

        Ignores soft-clipping, and flags a
        :param cigarTuples: A list of tuples
        :return: A tuple listing expanded cigar operators, as well as a boolean indicating if there is an INDEL in this read
        """

        cigarList = []
        hasIndel = False
        for cigarElement in cigarTuples:
            cigOp = cigarElement[0]
            # Ignore soft-clipped bases
            if cigOp == 4:
                continue
            # If there is an insertion or deletion in this read, indicate that
            if cigOp == 1 or cigOp == 2:
                hasIndel = True
            cigarList.extend([cigOp] * cigarElement[1])

        return tuple(cigarList), hasIndel

    def generatePileup(self, windowBuffer = 200):

        def processWindow(pos, coordinate):

            # Ignore non-variant positions
            if pos.alt:

                # Is this position within the capture space?
                if self._captureSpace is not None:
                    if chrom not in self._captureSpace or bisect.bisect_right(self._captureSpace[chrom], coordinate) % 2 != 1:
                        return

                # If there is a variant, we need to obtain some general characteristics to be used for filtering
                # First, store the sequence of the surrounding bases. This will be used to identify errors
                # due to homopolymers
                homoWindow = []
                windowPos = coordinate - refStart # Where, relative to the reference window, are we located?
                for j in range(windowPos - self._homopolymerWindow, windowPos + self._homopolymerWindow):
                    if windowPos == j:  # i.e. We are at the position we are currently examining, flag it so we have a reference point
                        homoWindow.append("M")  # It's me!
                    else:
                        homoWindow.append(refWindow[j])

                # Next, examine how "noisy" the neighbouring region is (i.e. how many candidiate variants there are)
                # Flag all nearby candidate variants. To avoid flagging the same variant twice, add this
                # candidate variant position to all variants behind it, and vise-versa
                if chrom not in self.candidateVar:
                    self.candidateVar[chrom] = sortedcontainers.SortedDict()

                nearbyVar = []
                for j in self.candidateVar[chrom].irange(coordinate - self._noiseWindow):
                    self.candidateVar[chrom][j].nearbyVar.append(pos)
                    nearbyVar.append(self.candidateVar[chrom][j])

                # Finally, save all these stats we have just generated inside the variant, to be examined during
                # filtering
                pos.processVariant(homoWindow, nearbyVar, self._noiseWindow)
                self.candidateVar[chrom][coordinate] = pos


        # Print status messages
        sys.stderr.write("\t".join([self._printPrefix, time.strftime('%X'), "Starting...\n"]))
        sys.stderr.write("\t".join([self._printPrefix, time.strftime('%X'), "Finding Candidate Variants\n"]))
        readsProcessed = 0
        lastPos = -1
        chrom = None

        refWindow = ""
        refStart = 0
        hasInsertion = False

        for read in self._inFile:

            # Prior to adding each base in this read onto the pileup, we need to analyze the current read,
            # and obtain general characteristics (family size, mapping quality etc)

            # Print out status messages
            readsProcessed += 1
            if readsProcessed % 100000 == 0:
                sys.stderr.write("\t".join([self._printPrefix, time.strftime('%X'), "Reads Processed:%s\n" % readsProcessed]))

            # Ignore unmapped reads
            if read.is_unmapped:
                continue

            # Ignore reads without a cigar string, as they are malformed or empty
            if not read.cigartuples:
                continue

            # Have we advanced positions? If so, we may need to change the loaded reference window, and process
            # positions which no more reads will be mapped to
            if read.reference_name != chrom or read.reference_start != lastPos:

                # Sanity check that the input BAM is sorted
                if read.reference_name == chrom and read.reference_start < lastPos:
                    sys.stderr.write("ERROR: The input BAM file does not appear to be sorted\n")
                    exit(1)

                # Process positions at which no more reads are expected to map to (assuming the input is actually sorted)
                # We do this to drastically reduce the memory footprint of the pileup, since we don't care about non-variant
                # positions very much
                try:
                    if read.reference_name != chrom and chrom is not None:

                        # If we have switched chromosomes, process all variants stored on the previous chromosome
                        posToProcess = self.pileup[chrom].keys()
                        for coordinate in posToProcess:
                            processWindow(self.pileup[chrom][coordinate], coordinate)
                        del self.pileup[chrom]
                        # Since this is the first read we are processing from this chromosome, we need to create the dictionary which
                        # will store all bases on this chromosome
                        self.pileup[read.reference_name] = sortedcontainers.SortedDict()

                    # Otherwise, check to see if previous positions fall outside the buffer window. If so, process them
                    elif chrom is not None:

                        posToProcess = tuple(self.pileup[chrom].irange(minimum=None, maximum=read.reference_start - self._pileupWindow))
                        for coordinate in posToProcess:
                            processWindow(self.pileup[chrom][coordinate], coordinate)
                        for coordinate in posToProcess:
                            del self.pileup[chrom][coordinate]
                    elif chrom is None:
                        self.pileup[read.reference_name] = sortedcontainers.SortedDict()

                except KeyError:  # i.e. All reads which mapped to this contig or previous positions failed QC. There are no positions to process
                    pass

                # Load the current reference window and it's position
                # If we have switched contigs, or moved outside the window that is currently buffered,
                # we need to re-buffer the reference
                readWindowStart = read.reference_start - windowBuffer
                if readWindowStart < 0:
                    readWindowStart = 0
                readWindowEnd = read.reference_end + windowBuffer  #TODO: Don't estimate soft-clipping, but instead calculate it

                if read.reference_name != chrom or readWindowStart < refStart or readWindowEnd > refEnd:
                    # To reduce the number of times this needs to be performed, obtain a pretty large buffer
                    refStart = readWindowStart - self._pileupWindow - self._homopolymerWindow
                    if refStart < 0:
                        refStart = 0
                    refEnd = readWindowStart + 5000
                    chrom = read.reference_name
                    refWindow = self._refGenome[chrom][refStart:refEnd].seq

                lastPos = read.reference_start

            # Does this read overlap the capture space? If not, ignore it
            # Note that this will keep reads which partially overlap the capture space
            # We will perform a more precise restricting of the regions which overlap the capture space later
            if not self._insideCaptureSpace(read):
                continue

            # Is this read valid? Check the naming scheme of the read, and identify each feature stored in the name
            try:
                nameElements = read.query_name.split(":")
                # Barcode is element 1. This is no longer needed, so ignore it
                # Parental strand is element 2.
                assert nameElements[1] == "+" or nameElements[1] == "-"
                rPosParent = nameElements[1] == "+"
                # Family size is element 3
                rFSize = int(nameElements[2])
                # Counter is 4. This is used to identify duplexes
                counter = nameElements[3]

            except (TypeError, IndexError, AssertionError):
                sys.stderr.write("ERROR: The names of the reads in this BAM file are not consistent with those generated by ProDuSe Collapse\n")
                sys.stderr.write("We expected something line \'TAATGCATCTTGATTTGGTTGCGAGTTGCAAT:+:207:0\', and instead saw %s\n" % (read.query_name))
                exit(1)

            # Obtain generic read characteristics
            # If clipoverlap was run on this BAM file, the originating strand of each base will be stored in a tag
            try:  # Just in case this read does not contain that tag
                rMapStrand = read.get_tag("co")
            except KeyError:  # i.e. The required tag is not present. Either these reads do not overlap at all, or clipoverlap was never run on the input BAM file
                if read.is_reverse:
                    rMapStrand = ["R"] * len(read.query_sequence)
                else:
                    rMapStrand = ["F"] * len(read.query_sequence)

            rMappingQual = read.mapping_quality

            # Convert the cigar to a more user-friendly format
            rCigar, rHasIndel = self.cigarToTuple(read.cigartuples)
            rCigarLength = len(rCigar) - 1
            # Does this read support an INDEL?
            if rHasIndel:
                pass  # TODO: Handle this case

            # If clipoverlap was performed on each read, we need to pull out the tag which indicates what base originated
            # from which read (in the case of clipoverlap)

            # Next, map all bases on this read to the reference
            # Ignoring soft-clipped bases

            # All positions covered by this read
            positions = []

            # The number of variants supported by this read
            numMismatch = 0

            # Iterate through all bases and cigar indexes
            cigarIndex = 0  # Keep this seperate to account for deletions
            readIterator = zip(read.query_alignment_sequence, read.query_alignment_qualities, rMapStrand[read.query_alignment_start:read.query_alignment_end])
            position = read.reference_start - 1

            try:
                while True:
                # for base, cigar, qual, mapStrand in zip(read.query_alignment_sequence, rCigar, read.query_alignment_qualities, rMapStrand):
                    cigar = rCigar[cigarIndex]
                    cigarIndex += 1
                    position += 1
                    # This operation is a deletion.
                    # Since deletions do not have quality scores or bases, we will have to specify placeholder values
                    if cigar == 2:
                        base = "-"
                        qual = 0
                        # mapStrand = mapStrand (aka, use the previous mapstrand to represent this event)
                    else:
                        base, qual, mapStrand = next(readIterator)

                    # Check to see if the next cigar operator is an insertion. If it is, we will include the insertion at this position
                    if rCigar[cigarIndex + 1] == 1:
                            qual = [qual]
                            mapStrand = [mapStrand]
                            while cigarIndex < rCigarLength and rCigar[cigarIndex + 1] == 1:
                                nBase, nQual, nMapStrand = next(readIterator)
                                base = base + nBase
                                qual.append(nQual)
                                mapStrand.append(nMapStrand)
                                cigarIndex += 1

                    # If this position has not been covered before, we need to generate a new pileup at this position
                    if position not in self.pileup[chrom]:
                        refBase = refWindow[position - refStart]
                        self.pileup[chrom][position] = Position(refBase)

                    pileupPos = self.pileup[chrom][position]

                    distFromEnd = min(position - read.reference_start, read.reference_end - position)

                    # Ignore bases that are "N"
                    if base == "N":
                        continue
                    isAlt = pileupPos.add(base, qual, rFSize, rPosParent, mapStrand, distFromEnd, rMappingQual, counter)

                    # Check to see if this position now has evidence of a variant
                    if isAlt:
                        numMismatch += 1
                    positions.append(pileupPos)

            except IndexError:
                pass
            # Store the number of total mismatches for this read at each variant
            for pos in positions:
                pos.addMismatchNum(numMismatch)

        # Now that we have finished processing all reads in the input file, process all remaining positions
        try:
            posToProcess = self.pileup[chrom].keys()
            for coordinate in posToProcess:
                processWindow(self.pileup[chrom][coordinate], coordinate)
        except KeyError:
            pass

        sys.stderr.write("\t".join([self._printPrefix, time.strftime('%X'), "Reads Processed:" + str(readsProcessed) + "\n"]))
        sys.stderr.write(
            "\t".join([self._printPrefix, time.strftime('%X'), "Candidate Variants Identified\n"]))


def isValidFile(file, parser):
    """
    Checks to ensure a specified file exists, and throws an error if it does not
    :param file: A string containing a filepath
    :param parser: An argparse.argumentparser() object
    :return: The variable "file", if the file exists
    :raises: parser.error() if the file does not exist
    """
    if os.path.exists(file):
        return file
    else:
        raise parser.error("Unable to locate %s. Please ensure the file exists, and try again" % file)


def validateArgs(args):
    """
    Checks that the specified set of arguments are valid
    :param args: A dictionary listing {argument: parameter}
    :return: A dictionary listing {argument: parameter} that have been validated
    """

    # Convert the dictionary into a list, to allow the arguments to be re-parsed
    listArgs = []
    for argument, parameter in args.items():

        if parameter is None or parameter is False or parameter == "None" or parameter == "False":
            continue
        # Something was provided as an argument
        listArgs.append("--" + argument)

        # If the parameter is a boolean, ignore it, as this will be reset once the arguments are re-parsed
        if parameter == "True":
            continue
        # If the parameter is a list, we need to add each element seperately
        if isinstance(parameter, list):
            for p in parameter:
                listArgs.append(str(p))
        else:
            listArgs.append(str(parameter))

    # Reparse the sanitized arguments
    # List requirements here
    parser = argparse.ArgumentParser(description="Identifies and calls variants")
    parser.add_argument("-c", "--config", metavar="INI", type=lambda x: isValidFile(x, parser),
                        help="An optional configuration file which can provide one or more arguments")
    parser.add_argument("-i", "--input", metavar="BAM", required=True, type=lambda x: isValidFile(x, parser),
                        help="Input post-collapse or post-clipoverlap BAM file")
    parser.add_argument("-o", "--output", metavar="VCF", required=True, help="Output VCF file")
    parser.add_argument("-u", "--unfiltered", metavar="VCF",
                        help="An additional output VCF file, which lists all candidate variants, including those that failed filters")
    parser.add_argument("-r", "--reference", metavar="FASTA", required=True, type=lambda x: isValidFile(x, parser),
                        help="Reference Genome, in FASTA format")
    parser.add_argument("-t", "--targets", metavar="BED", type=lambda x: isValidFile(x, parser),
                        help="A BED file containing regions in which to restrict variant calling")
    parser.add_argument("--min_depth", metavar="INT", type=int, default=3,
                        help="Minimum overall depth required to call a variant")
    parser.add_argument("--min_alt_molecules", metavar="INT", type=int, default=2,
                        help="Minimum number of molecules supporting an alternate allele required to call a variant")
    parser.add_argument("--strand_bias_threshold", metavar="FLOAT", type=float, default=0.05,
                        help="Any variants with a strand bias above this threshold will be filtered out")
    validatedArgs = parser.parse_args(listArgs)
    return vars(validatedArgs)


parser = argparse.ArgumentParser(description="Identifies and calls variants")
parser.add_argument("-c", "--config", metavar="INI", type=lambda x: isValidFile(x, parser), help="An optional configuration file which can provide one or more arguments")
parser.add_argument("-i", "--input", metavar="BAM", type=lambda x: isValidFile(x, parser), help="Input post-collapse or post-clipoverlap BAM file")
parser.add_argument("-o", "--output", metavar="VCF", help="Output VCF file, listing all variants which passed filters")
parser.add_argument("-u", "--unfiltered", metavar="VCF", help="An additional output VCF file, which lists all candidate variants, including those that failed filters")
parser.add_argument("-r", "--reference", metavar="FASTA", type=lambda x: isValidFile(x, parser), help="Reference Genome, in FASTA format")
parser.add_argument("-t", "--targets", metavar="BED", type=lambda x: isValidFile(x, parser), help="A BED file containing regions in which to restrict variant calling")
parser.add_argument("--min_depth", metavar="INT", type=int, help="Minimum overall depth required to call a variant")
parser.add_argument("--min_alt_molecules", metavar="INT", type=int, help="Minimum number of molecules supporting an alternate allele required to call a variant")
parser.add_argument("--strand_bias_threshold", metavar="FLOAT", type=float, help="Any variants with a strand bias above this threshold will be filtered out")


def main(args=None, sysStdin=None, printPrefix="PRODUSE-CALL\t"):
    if args is None:
        if sysStdin is None:
            args = parser.parse_args()
        else:
            args = parser.parse_args(sysStdin)
        args = vars(args)

    # If a config file was specified, parse the arguments from
    if args["config"] is not None:
        config = ConfigObj(args["config"])
        try:
            for argument, parameter in config["call"].items():
                if argument in args and not args[argument]:  # Aka this argument is used by call, and a parameter was not provided at the command line
                    args[argument] = parameter
        except KeyError:  # i.e. there is no section named "call" in the config file
            sys.stderr.write(
                "ERROR: Unable to locate a section named \'call\' in the config file \'%s\'\n" % (args["config"]))
            exit(1)

    args = validateArgs(args)
    pileup = PileupEngine(args["input"], args["reference"], args["targets"], printPrefix=printPrefix)

    # Find candidate variants
    pileup.generatePileup()
    pileup.filterAndWriteVariants(args["output"], args["unfiltered"], args["min_depth"], args["min_alt_molecules"], args["strand_bias_threshold"])


if __name__ == "__main__":
    main()